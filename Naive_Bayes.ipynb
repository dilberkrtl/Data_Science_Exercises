{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries: numpy, matplotlib, and seaborn for data visualization.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; \n",
    "sns.set()  # Seaborn's default styling for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `%matplotlib inline` allows the plots to be displayed directly below the code in Jupyter Notebooks.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing make_blobs from sklearn.datasets to generate a synthetic dataset for clustering.\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a synthetic dataset with 100 samples, each having 2 features (x and y), \n",
    "# and dividing the dataset into 2 centers (clusters) with standard deviation of 1.5.\n",
    "x, y = make_blobs(100, 2, centers=2, cluster_std=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the generated dataset. The 'c=y' argument colors the points according to their class labels.\n",
    "# 's=50' controls the size of the points, and 'cmap='RdBu'' sets the color map to red and blue.\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=50, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Gaussian Naive Bayes model from sklearn to perform classification.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Creating a Gaussian Naive Bayes classifier instance.\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on the generated data (x, y), where x is the features and y are the class labels.\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new data points for visualization using a random number generator.\n",
    "# These new points will be predicted by the trained Naive Bayes model to visualize decision boundaries.\n",
    "rng = np.random.RandomState(0)  # Ensures the same random values are generated each time.\n",
    "x_new = [-6, -14] + [14, 18] * rng.rand(1000, 2)  # Generating 1000 new random data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the class labels for the new data points using the trained model.\n",
    "y_new = model.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the original data points again and adding the new points with predicted class labels.\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()  # Get the current axis limits.\n",
    "plt.scatter(x_new[:, 0], x_new[:, 1], c=y_new, s=50, cmap='RdBu', alpha=0.2)  # Plot new points with predicted labels.\n",
    "plt.axis(lim)  # Reset the axis limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ã‡oklu Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 20 newsgroups dataset, which is a collection of approximately 20 different newsgroups.\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the categories (newsgroups) to filter from the dataset.\n",
    "categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and testing subsets for the selected categories.\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing one example of a document from the training set (document at index 5).\n",
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TfidfVectorizer for text data preprocessing. It converts the raw text into numerical features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Multinomial Naive Bayes classifier, commonly used for text classification tasks.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Importing make_pipeline, which helps create a sequence of data processing steps.\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline where:\n",
    "# 1. TfidfVectorizer converts the raw text into a numeric form (TF-IDF features).\n",
    "# 2. MultinomialNB applies the Naive Bayes algorithm to classify the text into one of the predefined categories.\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on the training dataset (train.data contains the text, train.target contains the labels).\n",
    "model.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set.\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model's performance by generating a confusion matrix to compare actual vs predicted labels.\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion matrix compares the true labels (test.target) with the predicted labels (labels).\n",
    "mat = confusion_matrix(test.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Seaborn to visualize the confusion matrix as a heatmap.\n",
    "# 'annot=True' annotates each cell in the heatmap with its numeric value, 'fmt=\"d\"' specifies integer format.\n",
    "# 'cbar=False' removes the color bar, which typically shows the scale of the heatmap.\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "\n",
    "# Adding labels to the axes.\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the category of a new text (string) using the trained model.\n",
    "def predict_category(s, train=train, model=model):\n",
    "    # 'model.predict()' classifies the input text and returns the predicted category index.\n",
    "    pred = model.predict([s])\n",
    "    # 'train.target_names' contains the list of category names; we use the predicted index to get the corresponding category.\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model with some example text inputs.\n",
    "predict_category('discussing islam vs atheism')  # Should predict one of the categories based on the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('determining the screen resolution')  # Another example to test text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
